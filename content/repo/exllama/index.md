---

title: Exllama
summary: An inference library for running local LLMs on modern consumer GPUs.
date: 2024-06-15
showData: false
# authors:
#   - admin
tags:
  - tag

home_weight: 10
showathome: true

external_link: https://github.com/kvcache-ai/Lexllama
doc_link: /docs/exllama

---
